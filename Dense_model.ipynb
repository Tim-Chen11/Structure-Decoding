{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3604f6e",
   "metadata": {},
   "source": [
    "Sure! Here's a full explanation of your image **in English**, covering the **four major architecture types of large language models (LLMs)**, what the colors mean, and how they differ.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  Four Types of LLM Architectures (as shown in the image)\n",
    "\n",
    "> LLMs can be divided into four main structures:\n",
    "\n",
    "* **Encoder-only**\n",
    "* **Decoder-only**\n",
    "* **Encoder-Decoder**\n",
    "* **Prefix LM** (can be viewed as a variant of Encoder-Decoder)\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… 1. **Encoder-only** (Green section)\n",
    "\n",
    "#### ğŸ”¹Key Features:\n",
    "\n",
    "* Uses **bidirectional self-attention**: every token can see both the left and right context.\n",
    "* Good for **understanding** tasks.\n",
    "\n",
    "#### ğŸ”¹Applications:\n",
    "\n",
    "* Mainly for **Natural Language Understanding (NLU)** tasks:\n",
    "\n",
    "  * Text classification\n",
    "  * Named entity recognition (NER)\n",
    "  * Question answering (QA)\n",
    "\n",
    "#### ğŸ”¹Examples:\n",
    "\n",
    "* **BERT**\n",
    "\n",
    "#### ğŸ”¹Pros:\n",
    "\n",
    "* Deep understanding of sentence context.\n",
    "\n",
    "#### ğŸ”¹Cons:\n",
    "\n",
    "* **Cannot generate text** (not autoregressive, no decoder).\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… 2. **Decoder-only** (Yellow section)\n",
    "\n",
    "#### ğŸ”¹Key Features:\n",
    "\n",
    "* Uses **left-to-right, unidirectional attention** (causal masking).\n",
    "* It's an **autoregressive model**: predicts the next token step by step.\n",
    "\n",
    "#### ğŸ”¹Applications:\n",
    "\n",
    "* Mostly used for **Natural Language Generation (NLG)** tasks:\n",
    "\n",
    "  * Chatbots\n",
    "  * Text completion\n",
    "  * Code generation\n",
    "  * Dialogue systems\n",
    "\n",
    "#### ğŸ”¹Examples:\n",
    "\n",
    "* **GPT**, **LLaMA**, **BLOOM**, **OPT**\n",
    "\n",
    "#### ğŸ”¹Pros:\n",
    "\n",
    "* Training and inference match (both autoregressive).\n",
    "* Great for **prompting**, **few-shot**, and **zero-shot** use.\n",
    "* Strong performance on generation tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… 3. **Encoder-Decoder** (Pink section)\n",
    "\n",
    "#### ğŸ”¹Key Features:\n",
    "\n",
    "* **Bidirectional attention** on the input (encoder)\n",
    "* **Unidirectional attention** on the output (decoder)\n",
    "* Standard **Seq2Seq** structure\n",
    "\n",
    "#### ğŸ”¹Applications:\n",
    "\n",
    "* Ideal for **sequence-to-sequence tasks**, such as:\n",
    "\n",
    "  * Machine translation\n",
    "  * Summarization\n",
    "  * Text-to-text generation\n",
    "\n",
    "#### ğŸ”¹Examples:\n",
    "\n",
    "* **T5**, **Flan-T5**, **BART**\n",
    "\n",
    "#### ğŸ”¹Pros:\n",
    "\n",
    "* Excellent understanding of the input\n",
    "* Generates more context-aware outputs\n",
    "\n",
    "#### ğŸ”¹Cons:\n",
    "\n",
    "* **Less efficient** during training\n",
    "* May underperform on pure text generation compared to decoder-only models\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… 4. **Prefix LM** (Purple section)\n",
    "\n",
    "#### ğŸ”¹Description:\n",
    "\n",
    "* A **hybrid** of Encoder-Decoder and Decoder-only\n",
    "* Uses special **masking** so that:\n",
    "\n",
    "  * The **prefix** input (context) is bidirectional\n",
    "  * The **generation** part is unidirectional\n",
    "\n",
    "#### ğŸ”¹Examples:\n",
    "\n",
    "* **GLM**\n",
    "* **U-PaLM**\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ Why Decoder-only is Often Chosen (Bottom section):\n",
    "\n",
    "* Based on **next-token prediction** â€” a simple, unified training objective\n",
    "* Can handle **many types of tasks** by just changing the prompt\n",
    "* Scales well to **long context lengths**\n",
    "* Easy to support few-shot and zero-shot learning\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” Summary Table\n",
    "\n",
    "| Model Type      | Attention Direction                                 | Best For            | Examples    | Text Generation |\n",
    "| --------------- | --------------------------------------------------- | ------------------- | ----------- | --------------- |\n",
    "| Encoder-only    | Bidirectional (input only)                          | Understanding (NLU) | BERT        | âŒ No            |\n",
    "| Decoder-only    | Unidirectional (leftâ†’right)                         | Generation (NLG)    | GPT, LLaMA  | âœ… Yes           |\n",
    "| Encoder-Decoder | Input: bidirectional<br>Output: unidirectional      | Seq2Seq tasks       | T5, BART    | âœ… Yes           |\n",
    "| Prefix LM       | Prefix: bidirectional<br>Generation: unidirectional | Unified prompting   | GLM, U-PaLM | âœ… Yes           |\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
