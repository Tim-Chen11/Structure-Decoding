{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eae27653",
   "metadata": {},
   "source": [
    "Decoding (Ëß£Á†Å) sampling strategies are the methods a language model uses to pick the next word (token) when generating text. Each strategy balances **quality**, **diversity**, and **speed** in different ways.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Greedy Search\n",
    "\n",
    "* **What it does**: Always pick the single most probable next token at each step.\n",
    "* **How it works**:\n",
    "\n",
    "  1. Model outputs a probability distribution over the vocabulary.\n",
    "  2. Choose the token with the highest probability.\n",
    "  3. Append it, then repeat.\n",
    "* **Pros**:\n",
    "\n",
    "  * Fast and simple.\n",
    "  * Deterministic (always gives the same result).\n",
    "* **Cons**:\n",
    "\n",
    "  * Can get stuck in repetitive or sub-optimal loops.\n",
    "  * Often lacks diversity and can produce bland text.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Beam Search\n",
    "\n",
    "* **What it does**: Keeps the top *k* candidate sequences (‚Äúbeams‚Äù) at each step instead of just one.\n",
    "* **How it works** (*beam size* = B):\n",
    "\n",
    "  1. Start with an empty sequence.\n",
    "  2. At each step, expand each current beam by all possible next tokens.\n",
    "  3. Score each new partial sequence (usually sum of log-probabilities).\n",
    "  4. Keep the top B sequences and discard the rest.\n",
    "  5. Repeat until sequences end (e.g., with an ‚Äú<eos>‚Äù token) or reach max length.\n",
    "* **Pros**:\n",
    "\n",
    "  * Balances exploration of multiple paths.\n",
    "  * Often finds higher-probability overall sequences than greedy.\n",
    "* **Cons**:\n",
    "\n",
    "  * More computationally expensive than greedy.\n",
    "  * Larger beams can still produce repetitive text and cost more time.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Top-k Sampling\n",
    "\n",
    "* **What it does**: Randomly sample the next token from only the *k* most probable tokens.\n",
    "* **How it works**:\n",
    "\n",
    "  1. Sort tokens by probability.\n",
    "  2. Keep only the top k tokens, set all others‚Äô probability to zero.\n",
    "  3. Renormalize the remaining probabilities.\n",
    "  4. Draw one token at random according to this new distribution.\n",
    "* **Pros**:\n",
    "\n",
    "  * Introduces randomness for more varied, creative outputs.\n",
    "  * Controls diversity by setting k (larger k ‚Üí more randomness).\n",
    "* **Cons**:\n",
    "\n",
    "  * If k is too small, output can still be repetitive.\n",
    "  * If k is too large, low-quality tokens may be chosen.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Top-p (Nucleus) Sampling\n",
    "\n",
    "* **What it does**: Randomly sample from the smallest set of tokens whose cumulative probability ‚â• p.\n",
    "* **How it works**:\n",
    "\n",
    "  1. Sort tokens by probability.\n",
    "  2. Find smallest set S so that sum(probabilities of S) ‚â• p (e.g. p=0.9).\n",
    "  3. Zero out all other tokens, renormalize, then sample.\n",
    "* **Pros**:\n",
    "\n",
    "  * Dynamically adapts the candidate set size based on model confidence.\n",
    "  * Often better quality/diversity tradeoff than fixed k.\n",
    "* **Cons**:\n",
    "\n",
    "  * Slightly more complex to implement.\n",
    "  * Choice of p still requires tuning.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Temperature Sampling\n",
    "\n",
    "* **What it does**: Adjusts the ‚Äúpeakiness‚Äù of the distribution before sampling.\n",
    "* **How it works**:\n",
    "\n",
    "  1. Divide the logits (pre-softmax scores) by a temperature T (>0).\n",
    "  2. Apply softmax to get new probabilities.\n",
    "\n",
    "     * **T < 1**: makes the distribution sharper (more greedy).\n",
    "     * **T > 1**: makes it flatter (more random).\n",
    "  3. Sample according to this adjusted distribution (often combined with top-k or top-p).\n",
    "* **Pros**:\n",
    "\n",
    "  * Simple knob to tune randomness.\n",
    "  * Can be combined with other methods.\n",
    "* **Cons**:\n",
    "\n",
    "  * Too high T ‚Üí incoherent text; too low T ‚Üí repetitive.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Best-of-*n* Sampling\n",
    "\n",
    "* **What it does**: Generate *n* complete outputs (usually with a stochastic method like top-k or top-p) and pick the highest-scoring one.\n",
    "* **How it works**:\n",
    "\n",
    "  1. Run the model *n* times, each time sampling a full sequence.\n",
    "  2. Score each sequence by its overall probability or a custom metric.\n",
    "  3. Return the sequence with the best score.\n",
    "* **Pros**:\n",
    "\n",
    "  * Improves quality by selecting the best among multiple tries.\n",
    "* **Cons**:\n",
    "\n",
    "  * *n* generations cost *n*√ó compute time.\n",
    "  * Doesn‚Äôt guarantee diversity if the metric favors similar beams.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Self-Consistency & Majority Vote\n",
    "\n",
    "* **What it does**: Similar to best-of-n, but focuses on consistent answers‚Äîespecially for tasks like reasoning or QA.\n",
    "* **How it works**:\n",
    "\n",
    "  1. Generate many (e.g. 20‚Äì100) samples with a high-temperature decoder.\n",
    "  2. Extract the final answer or decision from each sample.\n",
    "  3. Take the majority vote among answers (the most common one).\n",
    "* **Pros**:\n",
    "\n",
    "  * Reduces random errors in reasoning or classification tasks.\n",
    "  * Leverages collective ‚Äúwisdom‚Äù of multiple outputs.\n",
    "* **Cons**:\n",
    "\n",
    "  * Expensive (many generations).\n",
    "  * Requires a clear, extractable answer from each sample.\n",
    "\n",
    "---\n",
    "\n",
    "## Clarification Needed: ‚Äúkpt‚Äù\n",
    "\n",
    "I‚Äôm not familiar with **kpt** as a standard decoding strategy. Could you please clarify what you mean by ‚Äúkpt‚Äù? For example:\n",
    "\n",
    "* Is it an abbreviation for a specific method?\n",
    "* A variant of top-k or top-p?\n",
    "\n",
    "Once I know what ‚Äúkpt‚Äù refers to, I can explain it in the same detail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba334b4",
   "metadata": {},
   "source": [
    "Certainly! Here's a detailed explanation of **KPT Sampling** (short for **Temperature + Top-k + Top-p sampling**) in English:\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ KPT Sampling: Temperature + Top-k + Top-p Combined\n",
    "\n",
    "**KPT sampling** is a decoding strategy that **combines three techniques** to make text generation both high-quality and diverse:\n",
    "\n",
    "1. **Temperature scaling (T)**\n",
    "2. **Top-k filtering (K)**\n",
    "3. **Top-p (nucleus) sampling (P)**\n",
    "\n",
    "Rather than using just one of these methods, KPT applies all three in sequence.\n",
    "\n",
    "---\n",
    "\n",
    "### üîß How It Works ‚Äì Step by Step:\n",
    "\n",
    "1. **Temperature Scaling (`T`)**\n",
    "\n",
    "   * The model first outputs logits (raw scores for each token).\n",
    "   * These logits are divided by a **temperature value** `T`.\n",
    "\n",
    "     * If `T < 1`: Makes the distribution sharper ‚Üí model is more confident ‚Üí less randomness.\n",
    "     * If `T > 1`: Makes the distribution flatter ‚Üí more randomness and exploration.\n",
    "\n",
    "2. **Top-k Filtering (`K`)**\n",
    "\n",
    "   * From the temperature-adjusted scores, **keep only the top-k most likely tokens**.\n",
    "   * All other tokens are removed (set to zero probability).\n",
    "\n",
    "3. **Top-p (Nucleus) Filtering (`P`)**\n",
    "\n",
    "   * From the remaining tokens, select the **smallest set of tokens whose cumulative probability ‚â• p** (e.g., 0.9).\n",
    "   * Discard any other tokens not in this nucleus set.\n",
    "\n",
    "4. **Intersection + Renormalization**\n",
    "\n",
    "   * Take the **intersection** of the Top-k and Top-p sets.\n",
    "   * Renormalize their probabilities to sum to 1.\n",
    "\n",
    "5. **Sampling**\n",
    "\n",
    "   * Finally, **randomly select** the next token from this renormalized distribution.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages\n",
    "\n",
    "* **Highly controllable**: Each of the three parameters (k, p, T) gives you control over randomness, confidence, and diversity.\n",
    "* **Flexible**: Works well in both creative and factual tasks.\n",
    "* **Avoids bad extremes**: Top-k ensures quality; Top-p ensures adaptability; Temperature fine-tunes randomness.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Disadvantages\n",
    "\n",
    "* **More complex**: Tuning three hyperparameters together can be tricky.\n",
    "* **Slightly slower**: It requires more computation than using just one strategy (e.g., top-k alone).\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Summary\n",
    "\n",
    "> **KPT Sampling = Temperature Scaling + Top-k Filtering + Top-p Sampling**\n",
    "> This combined method filters out low-quality tokens, adapts to the model's uncertainty, and balances determinism and creativity. It is widely used in high-quality language models like GPT-style systems.\n",
    "\n",
    "Let me know if you‚Äôd like a diagram or code example to help visualize it!\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
