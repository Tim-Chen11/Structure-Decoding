{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4821d749",
   "metadata": {},
   "source": [
    "Check this website\n",
    "\n",
    "https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts\n",
    "\n",
    "\n",
    "Got it! Here's the same content as clear **bullet points** with indentation to reflect the hierarchy:\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“˜ **Mixture of Experts (MoE) - Table of Contents**\n",
    "\n",
    "* **What is Mixture of Experts?**\n",
    "\n",
    "* **The Experts**\n",
    "\n",
    "  * Dense Layers\n",
    "  * Sparse Layers\n",
    "\n",
    "* **What does an Expert Learn?**\n",
    "\n",
    "* **The Architecture of Experts**\n",
    "\n",
    "* **The Routing Mechanism**\n",
    "\n",
    "  * The Router\n",
    "  * Selection of Experts\n",
    "  * The Complexity of Routing\n",
    "\n",
    "* **Load Balancing**\n",
    "\n",
    "  * KeepTopK\n",
    "  * Token Choice\n",
    "  * Auxiliary Loss\n",
    "\n",
    "* **Expert Capacity**\n",
    "\n",
    "* **Simplifying MoE with the ...**\n",
    "\n",
    "  * The Switching Layer\n",
    "  * Capacity Factor\n",
    "  * Auxiliary Loss\n",
    "\n",
    "* **Mixture of Experts in Vision ...**\n",
    "\n",
    "  * Vision-MoE\n",
    "  * From Sparse to Soft MoE\n",
    "\n",
    "* **Active vs. Sparse Parameter...**\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
